{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b33758f-0371-4043-88ff-177d7bacea06",
   "metadata": {},
   "source": [
    "# PsychoDNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82761267-35f9-4e82-b21b-4cb03640ec1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from tensorflow.python.eager import context\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "import pylab as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_USE_FRONTEND'] = '1'\n",
    "# os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'\n",
    "# print(f\"Random seed set as {seed}\")\n",
    "os.environ[\"PYTHONHASHSEED\"] = '0'\n",
    "    \n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    \n",
    "    \n",
    "### set background seed\n",
    "seed = 42\n",
    "# setup_seed(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.experimental.numpy.random.seed(seed)\n",
    "# tf.random.uniform([1], seed=seed)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe63ce-78b9-42ed-bee1-617f4742cfec",
   "metadata": {},
   "source": [
    "## import processed 2d matrix data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bd2da9d-7e39-4811-bfd9-ed74fec215f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('./data_intermediate/x_train.npy',allow_pickle=True)\n",
    "y_train = np.load('./data_intermediate/y_train.npy',allow_pickle=True)\n",
    "print(x_train[0].shape)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_train = x_train.astype('float32') \n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c964fcc8-f6a6-463b-8def-e632365d604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.        ]\n",
      "  [0.04338772]\n",
      "  [0.17487054]\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [0.13080749]]\n",
      "\n",
      " [[0.82400584]\n",
      "  [0.4432482 ]\n",
      "  [0.8478585 ]\n",
      "  [0.66367406]\n",
      "  [0.7751856 ]\n",
      "  [0.15940285]\n",
      "  [0.11855265]]]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0][0:2])\n",
    "print(y_train[0][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6e20c-f33a-43d5-a83d-396c669bf506",
   "metadata": {},
   "source": [
    "## Define architecture of PsychoDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fd26b06-69de-43aa-a771-9a7526b411f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define bottleneck\n",
    "class BottleNeck(tf.keras.layers.Layer):\n",
    "    def __init__(self, growth_rate, drop_rate):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=4 * growth_rate,\n",
    "                                            kernel_size=(1, 1),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=growth_rate,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            padding=\"same\")\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)\n",
    "        \n",
    "        self.listLayers = [self.bn1,\n",
    "                           tf.keras.layers.Activation(\"relu\"),\n",
    "                           self.conv1,\n",
    "                           self.bn2,\n",
    "                           tf.keras.layers.Activation(\"relu\"),\n",
    "                           self.conv2,\n",
    "                           self.dropout]\n",
    "\n",
    "    def call(self, x):\n",
    "        tf.random.set_seed(seed)\n",
    "        y = x\n",
    "        for layer in self.listLayers.layers:\n",
    "            y = layer(y)\n",
    "        y = tf.keras.layers.concatenate([x,y], axis=-1)\n",
    "        return y\n",
    "# define dense block\n",
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, growth_rate, drop_rate=0.5):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.growth_rate = growth_rate\n",
    "        self.drop_rate = drop_rate\n",
    "        self.listLayers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.listLayers.append(BottleNeck(growth_rate=self.growth_rate, drop_rate=self.drop_rate))\n",
    "\n",
    "    def call(self, x):\n",
    "        tf.random.set_seed(seed)\n",
    "        for layer in self.listLayers.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# define transition\n",
    "class TransitionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=out_channels,\n",
    "                                           kernel_size=(1, 1),\n",
    "                                           strides=1,\n",
    "                                           padding=\"same\")\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                              strides=2,\n",
    "                                              padding=\"same\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tf.random.set_seed(seed)\n",
    "        x = self.bn(inputs)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# define dense net\n",
    "class DenseNet(tf.keras.Model):\n",
    "    def __init__(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=num_init_features,\n",
    "                                           kernel_size=(3, 3),\n",
    "                                           strides=1,\n",
    "                                           input_shape = (7,7,1),\n",
    "                                           padding=\"same\")\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.num_channels = num_init_features\n",
    "        self.dense_block_1 = DenseBlock(num_layers=block_layers[0], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        self.num_channels += growth_rate * block_layers[0]\n",
    "        self.num_channels = compression_rate * self.num_channels\n",
    "        self.transition_1 = TransitionLayer(out_channels=int(self.num_channels))\n",
    "        self.dense_block_2 = DenseBlock(num_layers=block_layers[1], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "        self.num_channels += growth_rate * block_layers[1]\n",
    "        self.num_channels = compression_rate * self.num_channels\n",
    "        self.transition_2 = TransitionLayer(out_channels=int(self.num_channels))\n",
    "        self.dense_block_3 = DenseBlock(num_layers=block_layers[2], growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc = tf.keras.layers.Dense(units=2,\n",
    "                                        activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tf.random.set_seed(seed)\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "\n",
    "        x = self.dense_block_1(x)\n",
    "        x = self.transition_1(x)\n",
    "        x = self.dense_block_2(x)\n",
    "        x = self.transition_2(x)\n",
    "        x = self.dense_block_3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b54eab0-f63a-4267-8325-f57b5f3e1c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dense_net_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_782 (Conv2D)          (None, 7, 7, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_782 (Bat (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "dense_block_102 (DenseBlock) (None, 7, 7, 56)          11736     \n",
      "_________________________________________________________________\n",
      "transition_layer_68 (Transit (None, 4, 4, 28)          1820      \n",
      "_________________________________________________________________\n",
      "dense_block_103 (DenseBlock) (None, 4, 4, 60)          15648     \n",
      "_________________________________________________________________\n",
      "transition_layer_69 (Transit (None, 2, 2, 30)          2070      \n",
      "_________________________________________________________________\n",
      "dense_block_104 (DenseBlock) (None, 2, 2, 54)          11520     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_34  (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 110       \n",
      "=================================================================\n",
      "Total params: 43,352\n",
      "Trainable params: 41,628\n",
      "Non-trainable params: 1,724\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = DenseNet(num_init_features=32, growth_rate=8, block_layers=[3,4,3], compression_rate=0.5, drop_rate=0.3)\n",
    "    model.build(input_shape=(None, 7, 7, 1))\n",
    " \n",
    "    # Adding this call to the call() method solves it all\n",
    "    model.call(Input(shape=(7, 7, 1)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053a7a3-e20c-4e96-babc-d541c5480b41",
   "metadata": {},
   "source": [
    "# training with 10-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82411414-5cb4-4db6-96aa-039bb8b9551c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5575 20866 21599 17767 16200]\n",
      "[[[0.10028079]\n",
      "  [0.0486789 ]\n",
      "  [0.19985205]\n",
      "  [0.35672265]\n",
      "  [0.16218597]\n",
      "  [0.0603992 ]\n",
      "  [0.22237274]]\n",
      "\n",
      " [[0.03977959]\n",
      "  [0.69922537]\n",
      "  [0.63637286]\n",
      "  [0.2642445 ]\n",
      "  [0.484491  ]\n",
      "  [0.36677998]\n",
      "  [0.21734653]]]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "### define parameters\n",
    "num_folds = 10\n",
    "batch_size = 256\n",
    "no_epochs = 200\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffle_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "x_train = x_train[shuffle_ix]\n",
    "y_train = y_train[shuffle_ix]\n",
    "print(shuffle_ix[:5])\n",
    "print(x_train[0][0:2])\n",
    "print(y_train[0][0:2])\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e08329aa-c80e-4f81-b36b-0c17a6c0a829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "class ParameterTraining(object):\n",
    "    def __init__(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate, no_epochs_1, no_epochs_2, lr_1, lr_2):\n",
    "        self.num_init_features = num_init_features\n",
    "        self.growth_rate = growth_rate\n",
    "        self.block_layers = block_layers\n",
    "        self.compression_rate = compression_rate\n",
    "        self.drop_rate = drop_rate\n",
    "        self.no_epochs_1 = no_epochs_1\n",
    "        self.no_epochs_2 = no_epochs_2\n",
    "        self.lr_1 = lr_1\n",
    "        self.lr_2 = lr_2\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        result_fin = pd.DataFrame()\n",
    "        for num_init_features in self.num_init_features:\n",
    "            print('****************** num_init_features **************************--》', num_init_features)\n",
    "            for growth_rate in self.growth_rate:\n",
    "                print('****************** growth_rate **************************--》', growth_rate)\n",
    "                for compression_rate in self.compression_rate:\n",
    "                    print('****************** compression_rate **************************--》', compression_rate)\n",
    "                    for drop_rate in self.drop_rate:\n",
    "                        print('****************** drop_rate **************************--》', drop_rate)\n",
    "                        start = time.time() ### estimate running time for a 10-fold cv of a group of parameters\n",
    "                        val_acc_per_fold = []\n",
    "                        val_loss_per_fold = []\n",
    "                        val_prec_per_fold = []\n",
    "                        val_rec_per_fold = []\n",
    "                        val_spec_per_fold = []\n",
    "                        val_mcc_per_fold = []\n",
    "                        val_auc_per_fold = []\n",
    "                        \n",
    "                        fold_no = 1\n",
    "                        \n",
    "                        for train, val in kfold.split(x_train, y_train):\n",
    "\n",
    "                            model = DenseNet(num_init_features=num_init_features, growth_rate=growth_rate, block_layers=block_layers, compression_rate=compression_rate, drop_rate=drop_rate)\n",
    "                            model.compile(loss='categorical_crossentropy',\n",
    "                                          optimizer=tf.keras.optimizers.Adam(lr=lr_1),\n",
    "                                          # optimizer=tf.keras.optimizers.SGD(lr=lr,momentum=0.9),\n",
    "                                          metrics=['accuracy',tf.keras.metrics.AUC(name='auc')])\n",
    "                            print('------------------------------------------------------------------------')\n",
    "                            print('num_init_features %d , growth_rate %d , compression_rate %.2f , drop_rate %.2f' % (num_init_features,growth_rate,compression_rate,drop_rate))\n",
    "                            print(f'Training for fold {fold_no} ...')\n",
    "                            print('No. of negative/positive samples in training %d / %d' %  (np.sum(np.argmax(y_train[train], axis=1)==0),np.sum(np.argmax(y_train[train], axis=1)==1)))\n",
    "                            print('No. of negative/positive samples in validation %d / %d' % (np.sum(np.argmax(y_train[val], axis=1)==0),np.sum(np.argmax(y_train[val], axis=1)==1)))\n",
    "                            history = model.fit(x_train[train], y_train[train],\n",
    "                                                batch_size=256,epochs=no_epochs_1,\n",
    "                                                validation_data=(x_train[val], y_train[val]),\n",
    "                                                verbose = 0, # verbose：日志显示, verbose = 0 为不在标准输出流输出日志信息, verbose = 1 为输出进度条记录, verbose = 2 为每个epoch输出一行记录\n",
    "                                                callbacks = EarlyStop\n",
    "                                               )\n",
    "                            model.compile(loss='categorical_crossentropy',\n",
    "                                          optimizer=tf.keras.optimizers.Adam(lr=lr_2),\n",
    "                                          metrics=['accuracy',tf.keras.metrics.AUC(name='auc')])\n",
    "                            history = model.fit(x_train[train], y_train[train],\n",
    "                                                batch_size=256,epochs=no_epochs_2,\n",
    "                                                validation_data=(x_train[val], y_train[val]),\n",
    "                                                verbose = 0, # verbose：日志显示, verbose = 0 为不在标准输出流输出日志信息, verbose = 1 为输出进度条记录, verbose = 2 为每个epoch输出一行记录\n",
    "                                                callbacks = EarlyStop\n",
    "                                               )\n",
    "                            y_pred_val = model.predict(x_train[val])\n",
    "                            y_pred_val = np.argmax(y_pred_val,axis=1)\n",
    "                            y_val = np.argmax(y_train[val],axis=1)\n",
    "                            val_prec_per_fold.append(metrics.precision_score(y_val, y_pred_val)) # precision\n",
    "                            val_rec_per_fold.append(metrics.recall_score(y_val, y_pred_val)) # recall\n",
    "                            val_mcc_per_fold.append(metrics.matthews_corrcoef(y_val, y_pred_val)) # mcc\n",
    "                            cm = metrics.confusion_matrix(y_val,y_pred_val) # confusion matrix\n",
    "                            val_spec_per_fold.append(cm[0,0]/(cm[0,0]+cm[0,1])) # specificity\n",
    "                            \n",
    "                            val_acc_per_fold.append(history.history['val_accuracy'][-1])\n",
    "                            val_loss_per_fold.append(history.history['val_loss'][-1])\n",
    "                            val_auc_per_fold.append(history.history['val_auc'][-1])\n",
    "                            fold_no += 1\n",
    "                         \n",
    "                        print('time (%s)' % (timeSince(start)))\n",
    "                        print('------------------------------------------------------------------------')\n",
    "                        print('Average scores for all folds:')\n",
    "                        print(f'> Val_Accuracy: %.4f (+- %.4f)' % (np.mean(val_acc_per_fold),np.std(val_acc_per_fold)))\n",
    "                        print(f'> Val_Loss: %.4f (+- %.4f)' % (np.mean(val_loss_per_fold),np.std(val_loss_per_fold)))        \n",
    "                        print('------------------------------------------------------------------------')\n",
    "        return model,val_acc_per_fold,val_prec_per_fold,val_rec_per_fold,val_spec_per_fold,val_mcc_per_fold,val_auc_per_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b7fe2-4ad6-4f4f-b0eb-7e369d9761a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# training with settled parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77873ea0-1711-4f5c-9c42-70412d2d0997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************** num_init_features **************************--》 32\n",
      "****************** growth_rate **************************--》 8\n",
      "****************** compression_rate **************************--》 0.5\n",
      "****************** drop_rate **************************--》 0.3\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 1 ...\n",
      "No. of negative/positive samples in training 11734 / 8318\n",
      "No. of negative/positive samples in validation 1275 / 953\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 2 ...\n",
      "No. of negative/positive samples in training 11683 / 8369\n",
      "No. of negative/positive samples in validation 1326 / 902\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 3 ...\n",
      "No. of negative/positive samples in training 11698 / 8354\n",
      "No. of negative/positive samples in validation 1311 / 917\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 4 ...\n",
      "No. of negative/positive samples in training 11712 / 8340\n",
      "No. of negative/positive samples in validation 1297 / 931\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 5 ...\n",
      "No. of negative/positive samples in training 11722 / 8330\n",
      "No. of negative/positive samples in validation 1287 / 941\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 6 ...\n",
      "No. of negative/positive samples in training 11690 / 8362\n",
      "No. of negative/positive samples in validation 1319 / 909\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 7 ...\n",
      "No. of negative/positive samples in training 11707 / 8345\n",
      "No. of negative/positive samples in validation 1302 / 926\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 8 ...\n",
      "No. of negative/positive samples in training 11747 / 8305\n",
      "No. of negative/positive samples in validation 1262 / 966\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 9 ...\n",
      "No. of negative/positive samples in training 11697 / 8355\n",
      "No. of negative/positive samples in validation 1312 / 916\n",
      "------------------------------------------------------------------------\n",
      "num_init_features 32 , growth_rate 8 , compression_rate 0.50 , drop_rate 0.30\n",
      "Training for fold 10 ...\n",
      "No. of negative/positive samples in training 11691 / 8361\n",
      "No. of negative/positive samples in validation 1318 / 910\n",
      "time (10m 12s)\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Val_Accuracy: 0.9599 (+- 0.0046)\n",
      "> Val_Loss: 0.1144 (+- 0.0128)\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_init_features = [32]\n",
    "growth_rate = [8]\n",
    "block_layers = [3,4,3]\n",
    "compression_rate = [0.5]\n",
    "drop_rate = [0.3]\n",
    "no_epochs_1 = 10\n",
    "no_epochs_2 = 200\n",
    "lr_1 = 1e-3\n",
    "lr_2 = 1e-4\n",
    "\n",
    "EarlyStop = keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 20, min_delta = 0, mode = 'auto')\n",
    "\n",
    "PT = ParameterTraining(num_init_features, growth_rate, block_layers, compression_rate, drop_rate, no_epochs_1, no_epochs_2, lr_1, lr_2)\n",
    "# Densenet = PT.train()\n",
    "PsychoDNet,acc,prec,rec,spec,mcc,auc = PT.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa388e-783a-4703-81c5-3dd809c979b4",
   "metadata": {},
   "source": [
    "## model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f64a075-17f7-401a-9302-aac1f496d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Val_Accuracy: 0.9599 (+- 0.0046)\n",
      "> Val_Precision: 0.9489 (+- 0.0159)\n",
      "> Val_Recall: 0.9556 (+- 0.0106)\n",
      "> Val_Specificity: 0.9630 (+- 0.0127)\n",
      "> Val_MCC: 0.9178 (+- 0.0088)\n",
      "> Val_AUC: 0.9915 (+- 0.0020)\n"
     ]
    }
   ],
   "source": [
    "print(f'> Val_Accuracy: %.4f (+- %.4f)' % (np.mean(acc),np.std(acc)))\n",
    "print(f'> Val_Precision: %.4f (+- %.4f)' % (np.mean(prec),np.std(prec)))\n",
    "print(f'> Val_Recall: %.4f (+- %.4f)' % (np.mean(rec),np.std(rec)))\n",
    "print(f'> Val_Specificity: %.4f (+- %.4f)' % (np.mean(spec),np.std(spec)))\n",
    "print(f'> Val_MCC: %.4f (+- %.4f)' % (np.mean(mcc),np.std(mcc)))\n",
    "print(f'> Val_AUC: %.4f (+- %.4f)' % (np.mean(auc),np.std(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8026bab-0be5-4fef-9835-28f912a3db81",
   "metadata": {},
   "source": [
    "### PsychoDNet prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fa2d696-162b-439a-ab4b-ff530c6573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sample_ix = np.random.permutation(np.arange(100))\n",
    "x_sample = x_train[sample_ix]\n",
    "y_sample = y_train[sample_ix]\n",
    "threshold = 0.0392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34dbc4df-20f2-40eb-99b5-baa374fbe875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> AUC: 1.0000 \n"
     ]
    }
   ],
   "source": [
    "y_pred_sample = PsychoDNet.predict(x_sample)\n",
    "print(f'> AUC: %.4f ' % (metrics.roc_auc_score(y_sample, y_pred_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bda618b-b233-41a9-bb1e-3c22fa9d2a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Accuracy: 1.000 \n",
      "> Precision: 1.000 \n",
      "> Recall: 1.000 \n",
      "> Specificity: 1.000 \n",
      "> MCC: 1.000 \n"
     ]
    }
   ],
   "source": [
    "y_pred_sample_binary = (y_pred_sample[:,1] >= threshold).astype(int)\n",
    "y_sample_binary = np.argmax(y_sample,axis=1)\n",
    "print(f'> Accuracy: %.3f ' % (metrics.accuracy_score(y_sample_binary, y_pred_sample_binary)))\n",
    "print(f'> Precision: %.3f ' % (metrics.precision_score(y_sample_binary, y_pred_sample_binary)))\n",
    "print(f'> Recall: %.3f ' % (metrics.recall_score(y_sample_binary, y_pred_sample_binary)))\n",
    "cm = metrics.confusion_matrix(y_sample_binary,y_pred_sample_binary) # confusion matrix\n",
    "print(f'> Specificity: %.3f ' % (cm[0,0]/(cm[0,0]+cm[0,1])))\n",
    "print(f'> MCC: %.3f ' % (metrics.matthews_corrcoef(y_sample_binary, y_pred_sample_binary)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.4",
   "language": "python",
   "name": "tensorflow-2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
